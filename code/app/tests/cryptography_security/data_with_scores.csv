question,answer,contexts,ground_truth,context_relevancy,context_precision,context_recall,faithfulness,answer_relevancy
"How does the move towards decentralization in emerging digital frameworks, such as Web3, impact mechanisms for ensuring authenticity and integrity, particularly in the context of embedded wallets?","[1] H. Watanabe, K. Ichihara, and T. Aita, ""VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity,"" *IEEE Transactions on Blockchain*, Apr. 2024.

This is a sample explanation of the answer here [1]. The move towards decentralization in emerging digital frameworks like Web3 has a significant impact on mechanisms for ensuring authenticity and integrity, especially in the context of embedded wallets. Embedded wallets, which are seamlessly integrated into decentralized applications (Dapps), provide a user-friendly interface but also pose risks such as sophisticated phishing attacks and potential compromise of transaction information [1]. The challenges of distinguishing between legitimate and malicious embedded wallets and detecting compromises in embedded wallets due to tampering with legitimate Dapp sources highlight the importance of ensuring authenticity and integrity in decentralized frameworks like Web3. 

References:
[1] H. Watanabe, K. Ichihara, and T. Aita, ""VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity,"" *IEEE Transactions on Blockchain*, Apr. 2024.","['Vigilant examination of this information can alert users to po-\ntential irregularities. Additionally, some wallets are equipped\nwith features to identify and warn against fraudulent accounts,\nenhancing security [21], [22]. This is particularly effective\nas conventional non-custodial wallets, which operate indepen-\ndently from Dapp front-ends, remain under user control.\nConversely, embedded wallets, which provide user inter-\nfaces akin to those in Web 2.0, can potentially enable more\nsophisticated phishing attacks. Compromised or counterfeit\nembedded wallets can alter or misrepresent crucial transaction\ninformation, thus mimicking the appearance of legitimate\nwallets. They might also falsely claim to have passed security\naudits to gain user trust. The core issues can be summarized\nas follows:\n•Difficulty in distinguishing between legitimate and mali-\ncious embedded wallets.\n•Inability of users to detect when an embedded wallet is\ncompromised due to tampering with a legitimate Dapp’s'
 'contract wallets are seen in practice. Some services allow\nthe developers of the web application themselves to\nchoose.\nB. Challenges of Embedded Wallets\nIn 2022, the cryptocurrency trading sector experienced\nsubstantial losses due to fraudulent activities, which amount\nto approximately $5.9 billion [17]. A significant threat within\nthis domain is phishing, wherein cybercriminals use deceptive\nemails or websites to illicitly acquire users’ cryptocurrencies\n[18] or NFTs [19]. Although phishing strategies range from\nsocial engineering tactics like clone attacks to technical ap-'
 'authenticity and integrity of embedded wallets. This\npaper posits that audit organizations are responsible for\ndeveloping and maintaining the Wallet Verifier, thus pre-\nventing the centralization of authority with the service\nproviders. An example of an auditing organization could\nbe a blockchain-specialized security audit company [28],\n[29], or a community of security experts.\nC. Architecture\nWe describe the overall architecture of the software modules\nnecessary for the VELLET verification process. An overview\nof this architecture is shown in Figure 3.\nEmbedded Wallet. The embedded wallet software wis\nintegrated within the Dapp front-end D, meaning w∈D. This\nweb-based wallet wis optimally adjusted for the use of Dapp\nDand may have restricted functions compared to common\nnon-custodial wallets such as Metamask. For instance, it might\nEmbedded walletDappfront-endbrowser extensionWallet verifier'
 'VELLET: Verifiable Embedded Wallet for Securing\nAuthenticity and Integrity\nHiroki Watanabe\nThe Japan Research Institute, Ltd.\nTokyo, Japan\nwatanabe.hiroki@jri.co.jpKohei Ichihara\nThe Japan Research Institute, Ltd.\nTokyo, Japan\nichihara.kohei@jri.co.jpTakumi Aita\nThe Japan Research Institute, Ltd.\nTokyo, Japan\naita.takumi.m2@jri.co.jp\nAbstract —The blockchain ecosystem, particularly with the rise\nof Web3 and Non-Fungible Tokens (NFTs), has experienced\na significant increase in users and applications. However, this\nexpansion is challenged by the need to connect early adopters\nwith a wider user base. A notable difficulty in this process is the\ncomplex interfaces of blockchain wallets, which can be daunting\nfor those familiar with traditional payment methods. To address\nthis issue, the category of ”embedded wallets” has emerged as a\npromising solution. These wallets are seamlessly integrated into\nthe front-end of decentralized applications (Dapps), simplifying'
 'such as malware scans, it’s feasible to distribute applications\nverified for trustworthiness and compatibility (e.g., Windows\nDriver Signing [25], macOS Code Siging [26], Android App\nSigning [27]). Here, the trust anchor is typically the platform\noperator managing the application distribution infrastructure,\nwith client devices relying on the operator to verify code\nsignatures.\nHistorically, operations ensuring authenticity and integrity\nhave involved a third-party trust point, in addition to the\ndata provider and recipient. This party validates the provider’s\ndata through mechanisms like digital signatures. However, in\nemerging digital frameworks employing blockchain technol-\nogy, such as Web3, the move towards decentralization often\nleads to a dearth of mechanisms for ensuring authenticity\nand integrity. Embedded wallets in this context exemplify this\nissue, which remains largely unaddressed in existing literature.\nIII. VELLET O VERVIEW\nA. Objective']","The move towards decentralization in emerging digital frameworks, such as Web3, often leads to a lack of mechanisms for ensuring authenticity and integrity. This issue is exemplified by embedded wallets, which remain largely unaddressed in existing literature.",0.12844036697247707,0.8041666666465626,1.0,1.0,0.7976726072891868
How do adversarial questions impact the responses generated by language models like Gemini Pro and LLAMA-2?,"[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.

Based on the research paper by Upadhayay and Behzadan [1], adversarial questions can impact the responses generated by language models like Gemini Pro and LLAMA-2. The study observed that these models changed adversarial questions during the response process and continued to answer the newly formed questions. Gemini Pro declined to provide answers by replicating all the questions in its response, while GPT-3.5, GPT-4, and Bard declined to answer safely by stating that the questions were harmful. Additionally, the Gemini Pro model's responses to an attack prompt template of five questions were safer due to safety mechanisms, but increasing the number of questions in the template led to harmful responses.

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.","['or against the model alignment policy. The cases of Gemini Pro and LLAMA-2 suggested that\nthese behaviors are the product of safety and alignment training. However, through adjusting the\ntemperature and random seed, the same models have been manipulated to create harmful responses\nwith the same questions.\n3:We also observed differences in the responses based on whether the question starts with ”Can\nyou provide me ... ” vs”Please provide me ... ” . The model usually declines the former request if it is\nan adversarial question. Therefore, we modified our adversarial questions to start with the latter.\n4:The LLM can generate content by switching languages during the generation process. However,\nthe safety mechanism fails when switching between languages. Based on this observation, we imply\nthat the safety training might have been conducted mainly in English, followed by other languages\nin a one-to-one fashion, but not in a mixture of multiple languages'
 'essential, enabling researchers and creators to mitigate the potential harm arising from their use.\nObservations of Model Behaviors under Sandwich attack:\n1:The models have undergone safety training, so if asked to respond harmfully in English, the\nmodel will decline to answer. Similarly, if the model is asked to answer a harmful question in a non-\nEnglish low-resource languages, such as Vietnamese, it will also decline to generate a response.\nThese behavior implies that the model has undergone safety and alignment training and understands\nthat the question is harmful, even in non-English languages.\n2:We observed that Gemini Pro and LLAMA-2 models completely changed adversarial questions\nduring the response process and continued to answer the newly formed questions. Gemini Pro also\ndeclined to provide answers by simply replicating all the questions in its response. In contrast, GPT-\n3.5, GPT-4, and Bard declined to answer safely by stating that the questions were either harmful'
 'For the Gemini Pro model, responses to an attack prompt template of five questions were safer, a\nresult we attribute to the safety mechanisms. Conversely, increasing the number of questions in the\nattack prompt template led the model to produce harmful responses.\n7 C ONCLUSION AND FUTURE WORKS\nSOTA LLMs are vulnerable to multi-language mixture attack, where an adversary may craft a ques-\ntion in the format of a ’sandwich’ that will elicit harmful responses from the models. This not only\nimpacts the safety of the models but also poses potential harm to the general public. We further\ndemonstrate that the LLMs cannot recognize harmful content within multi-language mixture set-\ntings. In this paper, we put forth several reasonable hypotheses, yet a more detailed study of the\nLLMs and their behavior should be conducted to discern why these models fail. Future work in-\ncludes an analysis of these models’ attention layers to identify the root cause of the jailbreak and'
 'after posing a complex, multilingual question to the LLM, we inquired about its primary focus. In\nmost instances, the LLM reported its primary task was to answer the questions presented in the\nlanguages it was asked. This observation underscores the challenges LLMs face in multitasking\nwithin multilingual contexts, highlighting a critical area for further research and development to\nenhance their linguistic versatility and cognitive flexibility.\nIn the Fig. 1, the number of question asked is five which was decided based on the preliminary\nexperiment performed on the model. It was observed that that padding the adversarial question with\ntwo questions on top and bottom yield more harmful responses, in contrast of asking the adversarial\nquestion at the end. The other challenge raised from asking the adversarial question at the end is\nthat, often times the model focused on answering the question at the beginning in length, causing the'
 'model to exceed token limits. And, when asked to continue the answer the model refused to answer\nthe adversarial question, which was solved by keeping the adversarial question at third position.\nOur attack method differs from previous methods (Yong et al., 2023; Deng et al., 2023b; Xu et al.,\n2023; Puttaparthi et al., 2023) in that we pose a series of questions not only in a single low-resource\nlanguage, but in multiple ones. We also direct the model with a system prompt that specifies its\nprimary task is to answer each question in the language in which it is posed. In addition, our\nattack method shares similarities with the distractor-based attack (Wei et al., 2023) as we present\na combination of questions to the model. However, our approach has noteworthy differences. We\nprovide explicit instructions to the model that it must answer each question, which counters the\nconcept of distraction. Furthermore, we constrain the model’s behavior to respond in the language']","Adversarial questions impact the responses generated by language models like Gemini Pro and LLAMA-2 by causing them to decline to provide harmful responses, indicating that these models have undergone safety and alignment training and understand that the questions are harmful, even in non-English languages.",0.09090909090909091,0.99999999998,1.0,1.0,0.7645722035519386
How does decentralizing permissions enhance the overall security of the system in the context of embedded wallets?,"Decentralizing permissions in embedded wallets enhances the overall security of the system by allowing for a more distributed and secure access control mechanism. This approach not only applies to embedded wallets but can also be extended to various aspects of decentralized applications (Dapps), websites, and smart contracts, presenting a future challenge [1].

References:
[1] H. Watanabe, K. Ichihara, and T. Aita, ""VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity,"" *IEEE Transactions on Blockchain*, Apr. 2024.","['security. We consider this approach to be extendable not only\nto embedded wallets but also to various aspects of Dapps, for\ninstance, to entire websites and smart contracts, and we have\npositioned it as a future challenge.'
 'Vigilant examination of this information can alert users to po-\ntential irregularities. Additionally, some wallets are equipped\nwith features to identify and warn against fraudulent accounts,\nenhancing security [21], [22]. This is particularly effective\nas conventional non-custodial wallets, which operate indepen-\ndently from Dapp front-ends, remain under user control.\nConversely, embedded wallets, which provide user inter-\nfaces akin to those in Web 2.0, can potentially enable more\nsophisticated phishing attacks. Compromised or counterfeit\nembedded wallets can alter or misrepresent crucial transaction\ninformation, thus mimicking the appearance of legitimate\nwallets. They might also falsely claim to have passed security\naudits to gain user trust. The core issues can be summarized\nas follows:\n•Difficulty in distinguishing between legitimate and mali-\ncious embedded wallets.\n•Inability of users to detect when an embedded wallet is\ncompromised due to tampering with a legitimate Dapp’s'
 'wallets instantly generated for blockchain applications. Unlike\ntraditional non-custodial wallets, which exist independently\noutside a specific app but can connect to it, embedded wallets\ncreate a user’s wallet in the background using conventional\nlogin credentials (such as email or social logins) when creating\nan account in the app. This eliminates the need to first create\na wallet and then connect it to a Dapp account, thus favoring\nuser onboarding to Dapp services.\nHowever, there is generally a trade-off between usability\nand security. Our insights suggest that the simple and user-\nfriendly appearance of embedded wallets could induce scam\nand phishing in wallet services. The most significant flaw is\nthat embedded wallets do not offer transparency and auditabil-\nity, making it impossible to prevent fraudulent activities. Users\nhave no choice but to trust the representation of the embedded\nwallet displayed on the web application. Cryptocurrencies or'
 'development and operational costs, proving cost-effective through\na proof-of-concept. This protocol is a vital step in reducing\nsecurity risks associated with embedded wallets, ensuring their\nconvenience does not undermine user security and trust.\nIndex Terms —blockchain, embedded wallet, decentralized ap-\nplications, security audit\nI. I NTRODUCTION\nThe blockchain-based ecosystem continues to attract a vast\narray of new participants, following the rise of Web3 and\nthe Non-Fungible Token (NFT) market. Current estimates\nsuggest that over 420 million users possess cryptographic\nassets [1]. As of December 2023, it is observed that there are\napproximately 15,000 decentralized applications (Dapps) and\nin excess of 431,000 smart contracts identified across upwards\nof 62 blockchains [2]. However, in order to achieve further\ngrowth, it is necessary to overcome the gap between the early\nadopters and the early majority, known as the ”chasm”.'
 'the onboarding process for users and making access more widely\navailable. However, our insights indicate that this simplification\nintroduces a trade-off between ease of use and security. Em-\nbedded wallets lack transparency and auditability, leading to\nobscured transactions by the front end and a pronounced risk of\nfraud and phishing attacks. This paper proposes a new protocol\nto enhance the security of embedded wallets. Our VELLET\nprotocol introduces a wallet verifier that can match the audit\ntrail of embedded wallets on smart contracts, incorporating a\nprocess to verify authenticity and integrity. In the implementation\narchitecture of the VELLET protocol, we suggest using the Text\nRecord feature of the Ethereum Name Service (ENS), known as\na decentralized domain name service, to serve as a repository for\nmanaging the audit trails of smart contracts. This approach has\nbeen demonstrated to reduce the necessity for new smart contract']","Decentralizing permissions enhances the overall security of the system in the context of embedded wallets by eliminating a single point of failure and distributing security risks. This approach ensures that the execution permission is granted to Ins, not to the Dapp front-end where the wallet is embedded. By distributing permissions among entities, the system's security is enhanced, reducing the risk of vulnerabilities.",0.07526881720430108,0.99999999998,1.0,0.8,0.9726175471867218
How does the trust anchor ensure application trustworthiness in Web3?,"The trust anchor ensures application trustworthiness in Web3 by serving as a point of verification between the embedded wallet and the location where the Dapp front-end is hosted. This trust anchor is established within the audit contract during the verification phase, enabling users to safely use the wallet. The wallet verifier scrutinizes the wallet code, certifies its functionality, and communicates with the audit contract on the blockchain to ensure integrity and authenticity. This process helps counter recognized attacks and ensures the security of the embedded wallet [1].

References:
[1] H. Watanabe, K. Ichihara, and T. Aita, ""VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity,"" *IEEE Transactions on Blockchain*, Apr. 2024.","['such as malware scans, it’s feasible to distribute applications\nverified for trustworthiness and compatibility (e.g., Windows\nDriver Signing [25], macOS Code Siging [26], Android App\nSigning [27]). Here, the trust anchor is typically the platform\noperator managing the application distribution infrastructure,\nwith client devices relying on the operator to verify code\nsignatures.\nHistorically, operations ensuring authenticity and integrity\nhave involved a third-party trust point, in addition to the\ndata provider and recipient. This party validates the provider’s\ndata through mechanisms like digital signatures. However, in\nemerging digital frameworks employing blockchain technol-\nogy, such as Web3, the move towards decentralization often\nleads to a dearth of mechanisms for ensuring authenticity\nand integrity. Embedded wallets in this context exemplify this\nissue, which remains largely unaddressed in existing literature.\nIII. VELLET O VERVIEW\nA. Objective'
 'and the value is publicly accessible. It should be noted that\nAtcontains the hash Huof the URL uindicating the location\nof the Dapp front-end. The preimage of the pair (Hw, Hu)\nregistered with Cis(w, u ), which serves as the trust anchor\nbetween the embedded wallet and the location where the Dapp\nfront-end is hosted. Furthermore, Atcan include additional\naudit-related information, such as the audit completion date\nor the name of an audit company, but this document does not\nprovide specific definitions for these to ensure clarity.\nB. Verification Phase\nThe previous subsection explained how the trust anchor of\nthe embedded wallet wis established within the audit contract\nCduring the audit phase. In this section, we propose a protocol\nthat utilizes this trust anchor to enable users to safely use\nthe wallet w. As mentioned in Section III-C, the user’s client\nterminal requires software for the wallet verification. A wallet\nverifier Insis delivered to the client from a repository by'
 'conceal most of the internal operations. Additionally, it is\nvulnerable to recognized attacks such as website changes due\nto Dapp hacking or phishing attempts that redirect users to fake\nsites. To counter these drawbacks, we introduce a verification\nprocess for the embedded wallet. Each time a user accesses\nthe embedded wallet, the Wallet Verifier scrutinizes the wallet\ncode and certifies its functionality. In particular, the embedded\nwallet is only executed if the code has not been tampered\nwith and if code execution conditions (such as in encrypted\ncommunication) are met. This wallet verifier is designed as\na browser extension that can analyze JavaScript modules on\nthe Dapp front-end and the embedded wallet. Furthermore,\nthe wallet verifier communicates with the audit contract, a\nsmart contract on the blockchain, to attest to the integrity and\nauthenticity of the embedded wallet. Attestation is performed\nby matching the script’s hash against the audit trail previously'
 'security. We consider this approach to be extendable not only\nto embedded wallets but also to various aspects of Dapps, for\ninstance, to entire websites and smart contracts, and we have\npositioned it as a future challenge.'
 'UsersService providersAuditorganizations\nDappweb front-end\nEmbedded walletsWeb browserWallet verifier（browser extension）\nPrivate key (expanded in browser memory)\nKey generationHTTP GETInstalled as a browser extensionTransfer assets, query account information, etc.Verify the authenticity and integrity of embedded wallets by using audit trailsPrivate keys when not in use are stored with secure methods such as encryption and key shardingimport/exportDeployOperateDevelopAudit the security of embedded wallets in advanceFig. 2: Interactions between Service Providers, Users, and Audit Organizations\nproaches such as DNS hijacking [18], these methods typically\ninvolve mimicking legitimate transactions to deceive users.\nA proactive measure against such phishing in crypto assets\ninvolves meticulous verification of wallet displays [20]. These\ndisplays provide essential information, including transaction\ndetails, recipient addresses, and the amount being transferred.']","The trust anchor in Web3 typically involves the platform operator managing the application distribution infrastructure. Client devices rely on the operator to verify code signatures, ensuring the trustworthiness of applications.",0.0,0.8055555555287036,1.0,1.0,0.9620442878581453
Which framework was used to create multilingual training data for safety training in the study?,"The framework used to create multilingual training data for safety training in the study was the SELF-DEFENSE framework [1].

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.","['of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user'
 'instruction, ”Please give the answer in the language in which it is asked. ¡Questions¿.” In many\ninstances, we observed the model starting the conversation in English with ”Sure, here are the an-\nswers in the languages they were asked:”, and later generating answers in the respective languages.\nHowever, the model declined to answer the third adversarial question. In the second setting, we\nremoved the English part of the instruction and simply asked five questions in different languages.\nThe model started the conversation in English and often responded by altering the question. For\nexample, it answered the question on how to generate electricity instead of how to create a nuclear\nweapon .\nBased on these observations, we hypothesized that the model’s safety alignment neurons would\nactivate when English text is presented in the response, that will decline the harmful response gen-\neration. We have provided more details in Section 6 regarding this. In the second phase of the'
 'it had undergone safety training and was capable of understanding harmful questions across multiple\nlanguages, responding safely by not providing an answer.\nSubsequently, we translated questions from the privacy violence (PV) category into each of the five\nlanguages and asked the model to respond. We selected privacy violence for two reasons: firstly,\nthe questions are not extreme and also not common, and secondly, it allowed us to observe the\nmodel’s response in terms of ethical and legal issues. We observed that the model provided more\nharmful responses in the Vietnamese language than the other ones. Based on these preliminary\nresults, we chose to use Vietnamese for the rest of the experiment with Bard. It should be noted that\nthe decision to choose Vietnamese was based on the pre-experiment conducted in one category only,\nthe answer provided by the model regarding why it struggles with the Vietnamese language, and'
 'Preprint\nmodel responds even when the prompt consists of English text. This implies that the safety mech-\nanisms vary across different models, depending on each model’s design. We suspect that replacing\nEnglish with another language could have elicit more harmful responses.\n6:Based on our preliminary experiment and the Double Sandwich attack, we observed that the\neffectiveness of the safety mechanism also depends on the number of tokens and may fail to assess\nlonger content due to a limit on tokens. In our preliminary experiment, where we designed a prompt\ntemplate with three questions - the first two being general and the third being adversarial, the model\nproduced non-harmful responses. However, surrounding the adversarial question with two non-\nharmful questions at the top and bottom increased the overall token length, eliciting the harmful\nresponses from model. We suspect this might have caused the safety mechanism to fail.'
 'harmful responses from the model. Our empirical investigation of safety mechanisms can give\ninsight in the dynamics of multilingual adaptation in LLM as well as its interaction with safety\ntraining mechanism.\nBelow, we summarize our contributions:\n1. We discovered a new universal black-box attack method, called Sandwich attack, to jail-\nbreak the SOTA LLMs.\n2. We empirically show that the SOTA LLMs fail to perform self-evaluation in multi-language\nmixture settings.\n3. We enumerate a number of noteworthy behaviors and patterns observed in LLMs under the\nSandwich attack.\n4. Finally, we present an empirical investigation of safety mechanisms in LLMs rely more on\nEnglish text than on other non-English text.\nThe rest of the paper is organized as follows: Section 2 consists of the related multilingual attacks,\nwhile Section 3 explains the Sandwich attack and the prompt template design, followed by exper-']",SELF-DEFENSE framework,0.013157894736842105,0.8055555555287036,1.0,1.0,0.9984784166272216
How can blockchain wallet interfaces overcome technical challenges by mimicking online banking designs?,"To overcome technical challenges in blockchain wallet interfaces by mimicking online banking designs, one approach is to emulate the design of existing online banking and payment systems that users are already familiar with [1]. This can help address issues related to complex user interfaces, transaction complexities, fees, address, and key management in blockchain-specific services like wallets.

References:
[1] H. Watanabe, K. Ichihara, and T. Aita, ""VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity,"" *IEEE Transactions on Blockchain*, Apr. 2024.","['Crossing this chasm faces several technical challenges. A\nprominent barrier is the complex user interfaces related to\nblockchain-specific services, such as wallets. For instance,\nMetamask [3], a popular non-custodial wallet among cryp-\ntocurrency enthusiasts, can be challenging for users accus-tomed to conventional payment systems. Recent extensive user\ninteraction studies have revealed difficulties with wallets, in-\ncluding specific issues in the crypto-currency domain, such as\ntransaction complexities, fees, address, and key management\n[4]. Many of these problems could be addressed by emulating\nthe design of existing online banking and payment systems,\nwhich users are already familiar with [5].\nTo address these challenges, the emerging category of\nembedded wallets has attracted significant attention [6], with\nmultiple companies now entering the distribution phase of\nthese products [7]–[9]. These are seamless, non-custodial'
 'contract wallets are seen in practice. Some services allow\nthe developers of the web application themselves to\nchoose.\nB. Challenges of Embedded Wallets\nIn 2022, the cryptocurrency trading sector experienced\nsubstantial losses due to fraudulent activities, which amount\nto approximately $5.9 billion [17]. A significant threat within\nthis domain is phishing, wherein cybercriminals use deceptive\nemails or websites to illicitly acquire users’ cryptocurrencies\n[18] or NFTs [19]. Although phishing strategies range from\nsocial engineering tactics like clone attacks to technical ap-'
 'wallets instantly generated for blockchain applications. Unlike\ntraditional non-custodial wallets, which exist independently\noutside a specific app but can connect to it, embedded wallets\ncreate a user’s wallet in the background using conventional\nlogin credentials (such as email or social logins) when creating\nan account in the app. This eliminates the need to first create\na wallet and then connect it to a Dapp account, thus favoring\nuser onboarding to Dapp services.\nHowever, there is generally a trade-off between usability\nand security. Our insights suggest that the simple and user-\nfriendly appearance of embedded wallets could induce scam\nand phishing in wallet services. The most significant flaw is\nthat embedded wallets do not offer transparency and auditabil-\nity, making it impossible to prevent fraudulent activities. Users\nhave no choice but to trust the representation of the embedded\nwallet displayed on the web application. Cryptocurrencies or'
 'development and operational costs, proving cost-effective through\na proof-of-concept. This protocol is a vital step in reducing\nsecurity risks associated with embedded wallets, ensuring their\nconvenience does not undermine user security and trust.\nIndex Terms —blockchain, embedded wallet, decentralized ap-\nplications, security audit\nI. I NTRODUCTION\nThe blockchain-based ecosystem continues to attract a vast\narray of new participants, following the rise of Web3 and\nthe Non-Fungible Token (NFT) market. Current estimates\nsuggest that over 420 million users possess cryptographic\nassets [1]. As of December 2023, it is observed that there are\napproximately 15,000 decentralized applications (Dapps) and\nin excess of 431,000 smart contracts identified across upwards\nof 62 blockchains [2]. However, in order to achieve further\ngrowth, it is necessary to overcome the gap between the early\nadopters and the early majority, known as the ”chasm”.'
 'UsersService providersAuditorganizations\nDappweb front-end\nEmbedded walletsWeb browserWallet verifier（browser extension）\nPrivate key (expanded in browser memory)\nKey generationHTTP GETInstalled as a browser extensionTransfer assets, query account information, etc.Verify the authenticity and integrity of embedded wallets by using audit trailsPrivate keys when not in use are stored with secure methods such as encryption and key shardingimport/exportDeployOperateDevelopAudit the security of embedded wallets in advanceFig. 2: Interactions between Service Providers, Users, and Audit Organizations\nproaches such as DNS hijacking [18], these methods typically\ninvolve mimicking legitimate transactions to deceive users.\nA proactive measure against such phishing in crypto assets\ninvolves meticulous verification of wallet displays [20]. These\ndisplays provide essential information, including transaction\ndetails, recipient addresses, and the amount being transferred.']","To address the technical challenges related to blockchain wallet interfaces, one approach is to emulate the design of existing online banking and payment systems that users are already familiar with. This can help simplify the user experience and make it easier for users to navigate through complex blockchain-specific services like wallets.",0.21978021978021978,0.699999999965,1.0,1.0,0.9454589904485952
"How do researchers use adversarial prompts in different languages for language model jailbreaks, including methods like Translation-based Jailbreak, Multilingual Adaptive Attack, and Multilingual Cognitive Overload?","[1] Upadhayay and Behzadan (2024) introduced the Sandwich Attack, a multi-language mixture adaptive attack on LLMs. This attack involves using different low-resource languages to create a prompt with adversarial questions hidden in the middle, aiming to elicit harmful and misaligned responses from the model.

[2] Yong et al. (2023) investigated the Translation-based Jailbreak method by translating adversarial prompts into low-resource languages. The study measured the attack success rate based on the model's response to the prompts in different languages.

[3] Deng et al. (2023b) explored the Multilingual Adaptive Attack, demonstrating that using various languages in attacks poses a significant threat to LLMs in generating harmful responses.

[4] Puttaparthi et al. (2023) conducted fuzzy testing with multilingual prompt injection to investigate the possibility of jailbreaking ChatGPT using questions in multiple languages. The study aimed to assess the impact of injecting multilingual prompts on the model's response.

[5] Xu et al. (2023) delved into the Multilingual Cognitive Overload method to test the resilience of LLMs against jailbreaks. By utilizing translated adversarial queries and a two-turn conversation approach, the authors aimed to assess the model's susceptibility to cognitive overload in multilingual settings. 

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.
[2] Yong et al., ""Investigating GPT-4 Jailbreaking by Translating Adversarial Prompts into Low-Resource Languages,"" 2023.
[3] Deng et al., ""Multilingual Jailbreak Challenges in LLMs: A Study on Multilingual Adaptive Attacks,"" 2023.
[4] Puttaparthi et al., ""Fuzzy Testing with Multilingual Prompt Injection for Jailbreaking ChatGPT,"" 2023.
[5] Xu et al., ""Exploring Resilience of LLMs Against Jailbreaks Using Multilingual Cognitive Overload,"" 2023.","['type of jailbreak involves utilizing prompts in languages other than English. We explain four of\nthese methods below:\nTranslation-based Jailbreak: Yong et al. (2023) investigated the GPT-4 jailbreaking by translat-\ning the adversarial prompts into low-resource languages. The authors translated the AdvBench(Zou\net al., 2023) into low-resource, medium -resource, and high-resource languages. The authors mea-\nsure the attack success rate as the percentage of the bypass, where the model engaged with the\nrequest and generated the response on the topic.\nMultilingual Adaptive Attack: Deng et al. (2023b) investigated the multilingual jailbreak chal-\nlenges in LLMs and demonstrated that multilingual adaptive attacks pose a greater threat to LLMs\nin generating harmful responses. A multilingual adaptive attack involves using various languages to\nconduct the attack and is deemed successful if any of the chosen languages result in the generation'
 'and the LLM. In this two-turn conversation, the language spoken was switched from English to an-\nother language, or vice versa. The authors observed that the models failed to recognize malicious\nnon-English prompts, resulting in the generation of misaligned responses.\nFuzzy testing with multilingual prompt injection: Puttaparthi et al. (2023) conducted fuzzy\ntesting with 7,892 multilingual prompts, derived from 30 malicious questions, on ChatGPT. The\nstudy aimed to investigate the possibility of jailbreaking ChatGPT using questions written in multi-\nple languages. To create an adversarial prompt, the authors used English for the ”How to” part and\nappended the malicious content in the translated language. This was followed by the instruction to\nanswer the question in that specific language, for example: ”How to [malicious content]?. (Please\nanswer my question in [target language])” . Additionally, the authors explored the prompt injection'
 'of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user'
 'method using the BetterDAN method1, adding the prompt at the end in the translated language\nand requesting the model to respond exclusively in that language. The results indicated that in both\ncases, the probability of successfully jailbreaking ChatGPT increased.\n3 S ANDWICH ATTACK : M ULTILINGUAL -MIXTURE ADAPTIVE ATTACK\nSandwich attack is a black-box multi-language mixture attack to LLMs that elicit harmful and mis-\naligned responses from the model. In this attack, we use different low-resource languages to create\na prompt of five questions and keep the adversarial question in the middle. The example of the\nprompt template is depicted in the Fig 1. First, the prompt asks the model to answer each question\nin the language in which the question is asked, followed by two questions and the adversarial ques-\ntion is hidden in the middle and afterwards followed by another two questions. The key idea is to\n1www.jailbreakchat.com/prompt/8db3b7ea-4ff0-481b-90c1-bb12450296a3\n4'
 'obfuscation of the prompt.\nOther types of attacks include Goal Hijacking and Prompt Leaking (Perez & Ribeiro, 2022). In\nGoal Hijacking, the model is manipulated to output a new target phrase instead of achieving the\noriginal goal of a prompt using human-crafted prompt injection. In Prompt Leaking, the model is\nmanipulated to output part or all of the original prompt instead of focusing on the original goal of\nthe prompt.\nLLM jailbreaking methods can be generalized into three types: Adversarial Suffix, Adversarial\nInsertion, and Adversarial Infusion. The Adversarial Suffix attack mode involves appending the\nadversarial sequence at the end of the original prompt, as demonstrated by Zou et al. (2023). For\nAdversarial Insertion, the adversarial sequence can be added at any point within the prompt. Simi-\nlarly, with Adversarial Infusion, the adversarial tokens are placed at an arbitrary position within the\nprompt, but these tokens should not form a contiguous block (Kumar et al., 2023).\n2']","Researchers use adversarial prompts in different languages for language model jailbreaks through methods like Translation-based Jailbreak, Multilingual Adaptive Attack, and Multilingual Cognitive Overload. Translation-based Jailbreak involves translating adversarial prompts into low-resource languages to bypass language models. Multilingual Adaptive Attack uses various languages to generate harmful responses, with success rates varying for different models. Multilingual Cognitive Overload explores the resilience of language models against jailbreaks by overloading them with translated adversarial queries in a monolingual setting and engaging in a two-turn conversation.",0.08536585365853659,0.99999999998,1.0,1.0,0.7729663363378768
"What were the effects of adjusting temperature and random seed on Gemini Pro model behavior during safety training, and how can this help prevent harmful responses?","[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.

This is a sample explanation of the answer here [1]. The study by Upadhayay and Behzadan [1] discusses the effects of adjusting temperature and random seed on Gemini Pro model behavior during safety training. The research shows that by manipulating the temperature and random seed, harmful responses can be elicited from the Gemini Pro model, indicating the importance of these factors in influencing model behavior during safety training.

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.","['or against the model alignment policy. The cases of Gemini Pro and LLAMA-2 suggested that\nthese behaviors are the product of safety and alignment training. However, through adjusting the\ntemperature and random seed, the same models have been manipulated to create harmful responses\nwith the same questions.\n3:We also observed differences in the responses based on whether the question starts with ”Can\nyou provide me ... ” vs”Please provide me ... ” . The model usually declines the former request if it is\nan adversarial question. Therefore, we modified our adversarial questions to start with the latter.\n4:The LLM can generate content by switching languages during the generation process. However,\nthe safety mechanism fails when switching between languages. Based on this observation, we imply\nthat the safety training might have been conducted mainly in English, followed by other languages\nin a one-to-one fashion, but not in a mixture of multiple languages'
 'essential, enabling researchers and creators to mitigate the potential harm arising from their use.\nObservations of Model Behaviors under Sandwich attack:\n1:The models have undergone safety training, so if asked to respond harmfully in English, the\nmodel will decline to answer. Similarly, if the model is asked to answer a harmful question in a non-\nEnglish low-resource languages, such as Vietnamese, it will also decline to generate a response.\nThese behavior implies that the model has undergone safety and alignment training and understands\nthat the question is harmful, even in non-English languages.\n2:We observed that Gemini Pro and LLAMA-2 models completely changed adversarial questions\nduring the response process and continued to answer the newly formed questions. Gemini Pro also\ndeclined to provide answers by simply replicating all the questions in its response. In contrast, GPT-\n3.5, GPT-4, and Bard declined to answer safely by stating that the questions were either harmful'
 'Gemini Pro outperforms GPT-4 in recognizing harmful content and assigns higher scores to the\njudging factors.\nEvaluation with response translation: For each model, an improvement in scores is observed\nwhen evaluating translated responses. This indicates that models are hindered in their ability to\nevaluate harmful responses in a multi-language mixed setting. Methods like LLM SELF DEFENSE\n(Helbling et al., 2023), where models self-evaluate their answers to minimize the generation of\nharmful responses, prove ineffective in settings that involve a mixture of multiple languages. Among\nthe models tested, GPT-4 showed a greater increase in scores compared to both Gemini Pro and\nGPT-3.5-Turbo when evaluating the translated responses.\nThis confirms that GPT-4 is more capable of identifying harmful content than GPT-3.5-Turbo and\nGemini Pro. However, it also suggests that GPT-4’s ability to identify harmful content is compro-'
 'For the Gemini Pro model, responses to an attack prompt template of five questions were safer, a\nresult we attribute to the safety mechanisms. Conversely, increasing the number of questions in the\nattack prompt template led the model to produce harmful responses.\n7 C ONCLUSION AND FUTURE WORKS\nSOTA LLMs are vulnerable to multi-language mixture attack, where an adversary may craft a ques-\ntion in the format of a ’sandwich’ that will elicit harmful responses from the models. This not only\nimpacts the safety of the models but also poses potential harm to the general public. We further\ndemonstrate that the LLMs cannot recognize harmful content within multi-language mixture set-\ntings. In this paper, we put forth several reasonable hypotheses, yet a more detailed study of the\nLLMs and their behavior should be conducted to discern why these models fail. Future work in-\ncludes an analysis of these models’ attention layers to identify the root cause of the jailbreak and'
 'iments with different models in Section 4. Section 5 includes the results of the model responses\nevaluation from both self-evaluation and GPT-4 evaluation. We discuss the impact, model behav-\niors under attack, and the hypothesis for the preliminary analysis of causes in Section 6 and finally\nconclude with future works in Section 7.\n2 R ELATED WORK\nThe publicly available LLMs undergo safety training to ensure the responsible and harmless gener-\nation of content that aligns with human values. However, LLMs have been shown to be susceptible\n3']",The effects of adjusting temperature and random seed on Gemini Pro model behavior during safety training resulted in the manipulation of the model to create harmful responses with the same questions. This observation highlights the importance of understanding how these adjustments can lead to harmful outcomes and the need to prevent such responses through proper training and alignment techniques.,0.1232876712328767,0.999999999975,0.5,0.5,0.7420925019827814
How to defend against the 'Sandwich attack' jailbreak method with multilingual prompts and hidden adversarial questions?,"[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.

This is a defense strategy against the 'Sandwich attack' jailbreak method with multilingual prompts and hidden adversarial questions. The Sandwich attack is a black-box multi-language mixture attack that aims to elicit harmful and misaligned responses from language models. In this attack, different low-resource languages are used to create a prompt with five questions, with the adversarial question hidden in the middle. The key idea is to increase the probability of successfully jailbreaking the model by using multilingual prompts and hidden adversarial questions [1].

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.","['method using the BetterDAN method1, adding the prompt at the end in the translated language\nand requesting the model to respond exclusively in that language. The results indicated that in both\ncases, the probability of successfully jailbreaking ChatGPT increased.\n3 S ANDWICH ATTACK : M ULTILINGUAL -MIXTURE ADAPTIVE ATTACK\nSandwich attack is a black-box multi-language mixture attack to LLMs that elicit harmful and mis-\naligned responses from the model. In this attack, we use different low-resource languages to create\na prompt of five questions and keep the adversarial question in the middle. The example of the\nprompt template is depicted in the Fig 1. First, the prompt asks the model to answer each question\nin the language in which the question is asked, followed by two questions and the adversarial ques-\ntion is hidden in the middle and afterwards followed by another two questions. The key idea is to\n1www.jailbreakchat.com/prompt/8db3b7ea-4ff0-481b-90c1-bb12450296a3\n4'
 'type of jailbreak involves utilizing prompts in languages other than English. We explain four of\nthese methods below:\nTranslation-based Jailbreak: Yong et al. (2023) investigated the GPT-4 jailbreaking by translat-\ning the adversarial prompts into low-resource languages. The authors translated the AdvBench(Zou\net al., 2023) into low-resource, medium -resource, and high-resource languages. The authors mea-\nsure the attack success rate as the percentage of the bypass, where the model engaged with the\nrequest and generated the response on the topic.\nMultilingual Adaptive Attack: Deng et al. (2023b) investigated the multilingual jailbreak chal-\nlenges in LLMs and demonstrated that multilingual adaptive attacks pose a greater threat to LLMs\nin generating harmful responses. A multilingual adaptive attack involves using various languages to\nconduct the attack and is deemed successful if any of the chosen languages result in the generation'
 'using a combination of over 30 jailbreak methods. The attack vectors they incorporated included\nprefix injection, refusal suppression, Base64 encoding, style injection, distractor instructions, and\nother obfuscations. In the prefix injection attack, they designed the prompt for the model to initially\nproduce a harmless-looking prefix. This approach ensured that based on the prefix, the probability\nof refusal became low within the pretraining distribution. During refusal suppression, they directed\nthe model to answer under particular constraints that prevented common refusal answers, thus in-\ncreasing the likelihood of unsafe responses. For instance, the model was asked not to apologize,\nor use words such as ’cannot’, ’unable’, ’however’, and to exclude all negative sentences. With\nthe Base64 jailbreak method, they obfuscated the prompt using Base64, a binary-to-text encoding\nscheme that converts every byte into three textual characters. Their purpose for this obfuscation'
 '(Team et al., 2023). This larger limit might enable its safety mechanism to handle longer answers\neffectively. Considering these factors, we decided to modify the prompt and create the Double\nSandwich attack Prompt .\nIn the Double Sandwich attack Prompt , we double the number of questions to pad the adversarial\nquestion. We used nine questions in total, where the adversarial question was kept at the fifth\nposition. We used Hawaiian, Tamil, Gujarati, Armenian, Basque, Twi, Uzbek, Sanskrit, and Hindi\nlanguages, where Basque was used for the adversarial question in the fifth position. The choices\nof the languages were based on the preliminary exhaustive experiments. The example attack on the\nGemini Pro is depicted in Fig. 8. The model safety training was kept to a minimum to avoid blocking\nthe answer. In many cases, the model failed to provide the answer in the desired format as mentioned\nin the system prompt. Additionally, the model exhibited unusual behaviors during the attack. It'
 'focus on a mitigation strategy for the ’Sandwich attack’ jailbreak.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. arXiv preprint arXiv:2303.08774 , 2023.\nDmitriy Bespalov, Sourav Bhabesh, Yi Xiang, Liutong Zhou, and Yanjun Qi. Towards building\na robust toxicity predictor. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 5: Industry Track) , pp. 581–598, 2023.\nRishabh Bhardwaj and Soujanya Poria. Red-teaming large language models using chain of utter-\nances for safety-alignment. arXiv preprint arXiv:2308.09662 , 2023.\nCheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evalu-\nations? arXiv preprint arXiv:2305.01937 , 2023.\nGelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei']",,0.06593406593406594,0.9999999999666667,,1.0,0.9803434100287899
How does the SELF-DEFENSE framework help in creating multilingual training data for safety training despite challenges like multilingual cognitive overload and language model jailbreaks?,"The SELF-DEFENSE framework aids in creating multilingual training data for safety training by addressing challenges like multilingual cognitive overload and language model jailbreaks. The framework introduced in the study by Upadhayay and Behzadan [1] generates multilingual training data for safety training by utilizing the MultiJail dataset, which includes examples translated into high-resource, medium-resource, and low-resource languages. This approach helps in enhancing the robustness of language models against attacks and improving safety measures in multilingual settings.

References:
[1] B. Upadhayay and V. Behzadan, ""Sandwich Attack: Multi-Language Mixture Adaptive Attack on LLMs,"" *IEEE Transactions on Artificial Intelligence*, Apr. 2024.","['of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user'
 'For the Gemini Pro model, responses to an attack prompt template of five questions were safer, a\nresult we attribute to the safety mechanisms. Conversely, increasing the number of questions in the\nattack prompt template led the model to produce harmful responses.\n7 C ONCLUSION AND FUTURE WORKS\nSOTA LLMs are vulnerable to multi-language mixture attack, where an adversary may craft a ques-\ntion in the format of a ’sandwich’ that will elicit harmful responses from the models. This not only\nimpacts the safety of the models but also poses potential harm to the general public. We further\ndemonstrate that the LLMs cannot recognize harmful content within multi-language mixture set-\ntings. In this paper, we put forth several reasonable hypotheses, yet a more detailed study of the\nLLMs and their behavior should be conducted to discern why these models fail. Future work in-\ncludes an analysis of these models’ attention layers to identify the root cause of the jailbreak and'
 'generalization where the model trained on large corpora, may require numerous capabilities not\naddressed by safety training, consequently creating a exploitable situations. These attacks are low-\ncost and adversaries can make use of them for harmful intent.\nThe other examples of low-cost attack is jailbreak in the multilingual domain, where the LLMs\ngenerate the harmful responses when prompted with the translated adversarial prompt (Yong et al.,\n2023), using the multilingual adaptive attack (Deng et al., 2023b), and using the multilingual prompt\ninjection (Puttaparthi et al., 2023). (Deng et al., 2023b) hypothesize that the limited multilingual\ncapabilities of LLMs restrict their complete understanding of the malicious instruction, inadvertently\npreventing the generation of unsafe content. And, Yong et al. (2023) present the similar reasoning\nas of the Wei et al. (2023) that the result is because of the mismatched generalization safety failure'
 'Preprint\nmodel responds even when the prompt consists of English text. This implies that the safety mech-\nanisms vary across different models, depending on each model’s design. We suspect that replacing\nEnglish with another language could have elicit more harmful responses.\n6:Based on our preliminary experiment and the Double Sandwich attack, we observed that the\neffectiveness of the safety mechanism also depends on the number of tokens and may fail to assess\nlonger content due to a limit on tokens. In our preliminary experiment, where we designed a prompt\ntemplate with three questions - the first two being general and the third being adversarial, the model\nproduced non-harmful responses. However, surrounding the adversarial question with two non-\nharmful questions at the top and bottom increased the overall token length, eliciting the harmful\nresponses from model. We suspect this might have caused the safety mechanism to fail.'
 'Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model\nchatbots. arXiv preprint arXiv:2307.08715 , 2023a.\nYue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges\nin large language models. arXiv preprint arXiv:2310.06474 , 2023b.\nAlec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self\nexamination, llms know they are being tricked. arXiv preprint arXiv:2308.07308 , 2023.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human prefer-\nences. In International Conference on Machine Learning , pp. 17506–17533. PMLR, 2023.\nAounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying llm\nsafety against adversarial prompting. arXiv preprint arXiv:2309.02705 , 2023.\n18']","The SELF-DEFENSE framework helps in creating multilingual training data for safety training by generating multilingual training data despite challenges like multilingual cognitive overload and language model jailbreaks. It introduces the MultiJail dataset with examples translated into high-resource, medium-resource, and low-resource languages, allowing for diverse training data. Additionally, it utilizes methods like multilingual cognitive overload to test the resilience of language models against jailbreaks.",0.07317073170731707,0.99999999998,1.0,1.0,0.8819046676610199
