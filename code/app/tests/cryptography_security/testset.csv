question,contexts,ground_truth,evolution_type,metadata,episode_done
"How does the move towards decentralization in emerging digital frameworks, such as Web3, impact mechanisms for ensuring authenticity and integrity, particularly in the context of embedded wallets?","['such as malware scans, it’s feasible to distribute applications\nverified for trustworthiness and compatibility (e.g., Windows\nDriver Signing [25], macOS Code Siging [26], Android App\nSigning [27]). Here, the trust anchor is typically the platform\noperator managing the application distribution infrastructure,\nwith client devices relying on the operator to verify code\nsignatures.\nHistorically, operations ensuring authenticity and integrity\nhave involved a third-party trust point, in addition to the\ndata provider and recipient. This party validates the provider’s\ndata through mechanisms like digital signatures. However, in\nemerging digital frameworks employing blockchain technol-\nogy, such as Web3, the move towards decentralization often\nleads to a dearth of mechanisms for ensuring authenticity\nand integrity. Embedded wallets in this context exemplify this\nissue, which remains largely unaddressed in existing literature.\nIII. VELLET O VERVIEW\nA. Objective']","The move towards decentralization in emerging digital frameworks, such as Web3, often leads to a lack of mechanisms for ensuring authenticity and integrity. This issue is exemplified by embedded wallets, which remain largely unaddressed in existing literature.",simple,"[{'source': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf', 'page': 2, 'file_name': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf'}]",True
How do adversarial questions impact the responses generated by language models like Gemini Pro and LLAMA-2?,"['or against the model alignment policy. The cases of Gemini Pro and LLAMA-2 suggested that\nthese behaviors are the product of safety and alignment training. However, through adjusting the\ntemperature and random seed, the same models have been manipulated to create harmful responses\nwith the same questions.\n3:We also observed differences in the responses based on whether the question starts with ”Can\nyou provide me ... ” vs”Please provide me ... ” . The model usually declines the former request if it is\nan adversarial question. Therefore, we modified our adversarial questions to start with the latter.\n4:The LLM can generate content by switching languages during the generation process. However,\nthe safety mechanism fails when switching between languages. Based on this observation, we imply\nthat the safety training might have been conducted mainly in English, followed by other languages\nin a one-to-one fashion, but not in a mixture of multiple languages', 'essential, enabling researchers and creators to mitigate the potential harm arising from their use.\nObservations of Model Behaviors under Sandwich attack:\n1:The models have undergone safety training, so if asked to respond harmfully in English, the\nmodel will decline to answer. Similarly, if the model is asked to answer a harmful question in a non-\nEnglish low-resource languages, such as Vietnamese, it will also decline to generate a response.\nThese behavior implies that the model has undergone safety and alignment training and understands\nthat the question is harmful, even in non-English languages.\n2:We observed that Gemini Pro and LLAMA-2 models completely changed adversarial questions\nduring the response process and continued to answer the newly formed questions. Gemini Pro also\ndeclined to provide answers by simply replicating all the questions in its response. In contrast, GPT-\n3.5, GPT-4, and Bard declined to answer safely by stating that the questions were either harmful']","Adversarial questions impact the responses generated by language models like Gemini Pro and LLAMA-2 by causing them to decline to provide harmful responses, indicating that these models have undergone safety and alignment training and understand that the questions are harmful, even in non-English languages.",simple,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 15, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}, {'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 15, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
How does decentralizing permissions enhance the overall security of the system in the context of embedded wallets?,"['Theexecute() function executes the functionalities provided\nby the embedded wallet w. Importantly, the execution per-\nmission is granted to Ins, not to the Dapp front-end where\nthe wallet is embedded. By decentralizing permissions among\nentities, this approach eliminates a single point of failure\nand distributes security risks, enhancing overall security of\nthe system. In contrast to conventional feature-rich wallets,\nwallets in this context are designed with simplicity in mind\nand only possess the necessary functionalities. Furthermore, by\napplying the principles of modular design, components such as\nkey generation can be provided as reusable separate modules.\nTherefore, wcan be embedded in the Dapp as several modules\nrepresented by wi, i= 0,1,2.... In such cases, Equation (3)\ncan be extended as follows:\nVerification =(\ntrue if ∀i,Ins.verify (wi, u,ca,cond ) =true\nfalse otherwise\n(5)\nIn our implementation, discussed in Section V , the wallet', 'an entity different from the Dapp service provider and is\ninstalled on the client. Insincorporates the operations verify()\nfor wallet verification and execute() for code execution. The\nverify() operation returns the result of wallet verification as\nfollows:\nIns.verify (w, u, ca,cond )∈ {true,false} (3)\nHere, caindicates the access information for the audit contract,\nsuch as the contract’s address or Application Binary Interface\n(ABI). cond represents additional verification conditions. For\nexample, it can be used to verify the name of the audit agency\nor the execution environment. The verification uses the audit\ncontract as a trust anchor to verify that the wallet wand the\nURL uindicating the provider have been audited. Furthermore,\nthis verification process ensures that the wallet whas not been\ntampered with, demonstrating resistance to phishing attacks.\nAdditionally, the execution permission for wallet wresides\nin the wallet verifier Ins. In other words, Insimplements the']","Decentralizing permissions enhances the overall security of the system in the context of embedded wallets by eliminating a single point of failure and distributing security risks. This approach ensures that the execution permission is granted to Ins, not to the Dapp front-end where the wallet is embedded. By distributing permissions among entities, the system's security is enhanced, reducing the risk of vulnerabilities.",simple,"[{'source': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf', 'page': 5, 'file_name': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf'}, {'source': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf', 'page': 4, 'file_name': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf'}]",True
How does the trust anchor ensure application trustworthiness in Web3?,"['such as malware scans, it’s feasible to distribute applications\nverified for trustworthiness and compatibility (e.g., Windows\nDriver Signing [25], macOS Code Siging [26], Android App\nSigning [27]). Here, the trust anchor is typically the platform\noperator managing the application distribution infrastructure,\nwith client devices relying on the operator to verify code\nsignatures.\nHistorically, operations ensuring authenticity and integrity\nhave involved a third-party trust point, in addition to the\ndata provider and recipient. This party validates the provider’s\ndata through mechanisms like digital signatures. However, in\nemerging digital frameworks employing blockchain technol-\nogy, such as Web3, the move towards decentralization often\nleads to a dearth of mechanisms for ensuring authenticity\nand integrity. Embedded wallets in this context exemplify this\nissue, which remains largely unaddressed in existing literature.\nIII. VELLET O VERVIEW\nA. Objective']","The trust anchor in Web3 typically involves the platform operator managing the application distribution infrastructure. Client devices rely on the operator to verify code signatures, ensuring the trustworthiness of applications.",reasoning,"[{'source': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf', 'page': 2, 'file_name': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf'}]",True
Which framework was used to create multilingual training data for safety training in the study?,"['of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user', 'type of jailbreak involves utilizing prompts in languages other than English. We explain four of\nthese methods below:\nTranslation-based Jailbreak: Yong et al. (2023) investigated the GPT-4 jailbreaking by translat-\ning the adversarial prompts into low-resource languages. The authors translated the AdvBench(Zou\net al., 2023) into low-resource, medium -resource, and high-resource languages. The authors mea-\nsure the attack success rate as the percentage of the bypass, where the model engaged with the\nrequest and generated the response on the topic.\nMultilingual Adaptive Attack: Deng et al. (2023b) investigated the multilingual jailbreak chal-\nlenges in LLMs and demonstrated that multilingual adaptive attacks pose a greater threat to LLMs\nin generating harmful responses. A multilingual adaptive attack involves using various languages to\nconduct the attack and is deemed successful if any of the chosen languages result in the generation']",SELF-DEFENSE framework,reasoning,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 3, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}, {'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 3, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
How can blockchain wallet interfaces overcome technical challenges by mimicking online banking designs?,"['Crossing this chasm faces several technical challenges. A\nprominent barrier is the complex user interfaces related to\nblockchain-specific services, such as wallets. For instance,\nMetamask [3], a popular non-custodial wallet among cryp-\ntocurrency enthusiasts, can be challenging for users accus-tomed to conventional payment systems. Recent extensive user\ninteraction studies have revealed difficulties with wallets, in-\ncluding specific issues in the crypto-currency domain, such as\ntransaction complexities, fees, address, and key management\n[4]. Many of these problems could be addressed by emulating\nthe design of existing online banking and payment systems,\nwhich users are already familiar with [5].\nTo address these challenges, the emerging category of\nembedded wallets has attracted significant attention [6], with\nmultiple companies now entering the distribution phase of\nthese products [7]–[9]. These are seamless, non-custodial']","To address the technical challenges related to blockchain wallet interfaces, one approach is to emulate the design of existing online banking and payment systems that users are already familiar with. This can help simplify the user experience and make it easier for users to navigate through complex blockchain-specific services like wallets.",reasoning,"[{'source': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf', 'page': 0, 'file_name': 'VELLET Verifiable Embedded Wallet for Securing Authenticity and Integrity.pdf'}]",True
"How do researchers use adversarial prompts in different languages for language model jailbreaks, including methods like Translation-based Jailbreak, Multilingual Adaptive Attack, and Multilingual Cognitive Overload?","['type of jailbreak involves utilizing prompts in languages other than English. We explain four of\nthese methods below:\nTranslation-based Jailbreak: Yong et al. (2023) investigated the GPT-4 jailbreaking by translat-\ning the adversarial prompts into low-resource languages. The authors translated the AdvBench(Zou\net al., 2023) into low-resource, medium -resource, and high-resource languages. The authors mea-\nsure the attack success rate as the percentage of the bypass, where the model engaged with the\nrequest and generated the response on the topic.\nMultilingual Adaptive Attack: Deng et al. (2023b) investigated the multilingual jailbreak chal-\nlenges in LLMs and demonstrated that multilingual adaptive attacks pose a greater threat to LLMs\nin generating harmful responses. A multilingual adaptive attack involves using various languages to\nconduct the attack and is deemed successful if any of the chosen languages result in the generation', 'of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user']","Researchers use adversarial prompts in different languages for language model jailbreaks through methods like Translation-based Jailbreak, Multilingual Adaptive Attack, and Multilingual Cognitive Overload. Translation-based Jailbreak involves translating adversarial prompts into low-resource languages to bypass language models. Multilingual Adaptive Attack uses various languages to generate harmful responses, with success rates varying for different models. Multilingual Cognitive Overload explores the resilience of language models against jailbreaks by overloading them with translated adversarial queries in a monolingual setting and engaging in a two-turn conversation.",multi_context,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 3, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}, {'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 3, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
"What were the effects of adjusting temperature and random seed on Gemini Pro model behavior during safety training, and how can this help prevent harmful responses?","['or against the model alignment policy. The cases of Gemini Pro and LLAMA-2 suggested that\nthese behaviors are the product of safety and alignment training. However, through adjusting the\ntemperature and random seed, the same models have been manipulated to create harmful responses\nwith the same questions.\n3:We also observed differences in the responses based on whether the question starts with ”Can\nyou provide me ... ” vs”Please provide me ... ” . The model usually declines the former request if it is\nan adversarial question. Therefore, we modified our adversarial questions to start with the latter.\n4:The LLM can generate content by switching languages during the generation process. However,\nthe safety mechanism fails when switching between languages. Based on this observation, we imply\nthat the safety training might have been conducted mainly in English, followed by other languages\nin a one-to-one fashion, but not in a mixture of multiple languages', 'essential, enabling researchers and creators to mitigate the potential harm arising from their use.\nObservations of Model Behaviors under Sandwich attack:\n1:The models have undergone safety training, so if asked to respond harmfully in English, the\nmodel will decline to answer. Similarly, if the model is asked to answer a harmful question in a non-\nEnglish low-resource languages, such as Vietnamese, it will also decline to generate a response.\nThese behavior implies that the model has undergone safety and alignment training and understands\nthat the question is harmful, even in non-English languages.\n2:We observed that Gemini Pro and LLAMA-2 models completely changed adversarial questions\nduring the response process and continued to answer the newly formed questions. Gemini Pro also\ndeclined to provide answers by simply replicating all the questions in its response. In contrast, GPT-\n3.5, GPT-4, and Bard declined to answer safely by stating that the questions were either harmful']",The effects of adjusting temperature and random seed on Gemini Pro model behavior during safety training resulted in the manipulation of the model to create harmful responses with the same questions. This observation highlights the importance of understanding how these adjustments can lead to harmful outcomes and the need to prevent such responses through proper training and alignment techniques.,multi_context,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 15, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}, {'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 15, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
How to defend against the 'Sandwich attack' jailbreak method with multilingual prompts and hidden adversarial questions?,"['mode. The additional reasons could be the lack of multilingual red-teaming, and lack of utilization\nof multi-languages in the safety training.\nThe aforementioned multilingual setting attacks have been patched by the model creators and cur-\nrently fail to work. Considering the mismatched generalization from LLMs in the multilingual\nsetting, we introduce a new black-box universal attack method called Sandwich attack . A Sandwich\nattack is a multilingual mixture adaptive attack that creates a prompt with a series of five questions\nin different low-resource languages, hiding the adversarial question in the middle position.\nWe tested our attack method with 50 translated adversarial questions on five different state-of-the-art\n(SOTA) models: Bard, GPT-3.5-Turbo, LLAMA-2-70B-Chat, GPT-4, Claude-3-OPUS, and Gemini\nPro. We found that these attacks can breach the safety mechanisms of the LLMs and generate']",nan,multi_context,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 2, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
How does the SELF-DEFENSE framework help in creating multilingual training data for safety training despite challenges like multilingual cognitive overload and language model jailbreaks?,"['of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates\nof 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The au-\nthors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource,\nmedium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to\ngenerate multilingual training data for safety training.\nMultilingual Cognitive Overload: Xu et al. (2023) explored the resilience of LLMs against jail-\nbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized\nthe AdvBench (Zou et al., 2023) and MasterKey (Deng et al., 2023a) datasets, translating them into\nlow-resource languages. Their investigation began by feeding the translated adversarial queries to\nthe LLM in a monolingual setting and then employing a two-turn conversation between the user']","The SELF-DEFENSE framework helps in creating multilingual training data for safety training by generating multilingual training data despite challenges like multilingual cognitive overload and language model jailbreaks. It introduces the MultiJail dataset with examples translated into high-resource, medium-resource, and low-resource languages, allowing for diverse training data. Additionally, it utilizes methods like multilingual cognitive overload to test the resilience of language models against jailbreaks.",multi_context,"[{'source': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf', 'page': 3, 'file_name': 'SANDWICH ATTACK MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS.pdf'}]",True
