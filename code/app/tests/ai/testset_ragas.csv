question,contexts,ground_truth,evolution_type,metadata,episode_done
Where will the PSLV-C56 mission be launched from?,"['When is the scheduled\nlaunch date and time for\nthe PSLV-C56 mission,\nand where will it be\nlaunched from?High answer relevance : The PSLV-C56 mission is scheduled to be launched on Sunday, 30\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\nCentre, Sriharikota, Andhra Pradesh, India.\nLow answer relevance : The scheduled launch date and time for the PSLV-C56 mission have\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\nlaunch a satellite into orbit to study weather patterns.\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\nQuestion Context\nWhen was the Chimnabai\nClock Tower completed,\nand who was it named af-\nter?High context relevance : The Chimnabai Clock Tower, also known as the Raopura Tower, is\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of']","The PSLV-C56 mission will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India.",simple,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 7}]",True
How are short extractive answers typically evaluated in natural language processing tasks?,"['tion answering is another common evaluation task,\nbut usually only datasets with short extractive an-\nswers are considered, which may not be represen-\ntative of how the system will be used.\nTo address these issues, in this paper we present\nRAGA S1, a framework for the automated assess-\n1RAGA Sis available at https://github.com/\nexplodinggradients/ragas .arXiv:2309.15217v1  [cs.CL]  26 Sep 2023']","Short extractive answers in natural language processing tasks are typically evaluated through common evaluation tasks, but these datasets may not fully represent how the system will be used.",simple,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 0}]",True
What historical event did the Manhattan Project start in WWII?,"['Question Context Answer\nWho directed the film Op-\npenheimer and who stars\nas J. Robert Oppenheimer\nin the film?Oppenheimer is a 2023 biographical thriller film written\nand directed by Christopher Nolan. Based on the 2005\nbiography American Prometheus by Kai Bird and Mar-\ntin J. Sherwin, the film chronicles the life of J. Robert\nOppenheimer, a theoretical physicist who was pivotal in\ndeveloping the first nuclear weapons as part of the Man-\nhattan Project, and thereby ushering in the Atomic Age.\nCillian Murphy stars as Oppenheimer, with Emily Blunt\nas Oppenheimer’s wife Katherine ""Kitty"" Oppenheimer.High Faithfulness : Christopher\nNolan directed the film Oppen-\nheimer. Cillian Murphy stars as J.\nRobert Oppenheimer in the film.\nLow Faithfulness : James\nCameron directed the film Op-\npenheimer. Tom Cruise stars as J.\nRobert Oppenheimer in the film.\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\nQuestion Answer\nWhen is the scheduled\nlaunch date and time for']",The Manhattan Project started during World War II.,reasoning,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 7}]",True
Which city is the City of Light?,"['Given a question and answer, create one\nor more statements from each sentence\nin the given answer.\nquestion: [question]\nanswer: [answer]\nwhere [question] and [answer] refer to the\ngiven question and answer. For each statement si\n2https://platform.openai.com\n3To help clarify the task, we include a demonstration as\npart of the prompt. This demonstration is not explicitly shown\nin the listing of the prompts throughout this paper.inS, the LLM determines if sican be inferred from\nc(q)using a verification function v(si, c(q)). This\nverification step is carried out using the following\nprompt:\nConsider the given context and following\nstatements, then determine whether they\nare supported by the information present\nin the context. Provide a brief explana-\ntion for each statement before arriving\nat the verdict (Yes/No). Provide a final\nverdict for each statement in order at the\nend in the given format. Do not deviate\nfrom the specified format.\nstatement: [statement 1]\n...']","The City of Light refers to Paris, France.",multi_context,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 2}]",True
How are confidence levels in tokens assessed in language generation models for factuality and truthfulness?,"['Mitchell, 2023). Other approaches rely on linking\nthe generated responses to facts from an external\nknowledge base (Min et al., 2023), but this is not\nalways possible.\nYet another strategy is to inspect the probabili-\nties assigned to individual tokens, where we would\nexpect the model to be less confident in halluci-\nnated answers than in factual ones. For instance,\nBARTScore (Yuan et al., 2021) estimates factuality\nby looking at the conditional probability of the gen-\nerated text given the input. Kadavath et al. (2022)\nuse a variation of this idea. Starting from the ob-\nservation that LLMs provide well-calibrated proba-\nbilities when answering multiple-choice questions,\nthey essentially convert the problem of validating\nmodel generated answers into a multiple-choice\nquestion which asks whether the answer is true or\nfalse. Rather than looking at the output probabil-\nities, Azaria and Mitchell (2023) propose to train\na supervised classifier on the weights from one of', 'the hidden layers of the LLM, to predict whether a\ngiven statement is true or not. While the approach\nperforms well, the need to access the hidden states\nof the model makes it unsuitable for systems that\naccess LLMs through an API.\nFor models that do not provide access to token\nprobabilities, such as ChatGPT and GPT-4, differ-\nent methods are needed. SelfCheckGPT (Manakul\net al., 2023) addresses this problem by instead sam-\npling multiple answers. Their core idea is thatfactual answers are more stable: when an answer is\nfactual, we can expect that different samples will\ntend to be semantically similar, whereas this is less\nlikely to be the case for hallucinated answers.\nAutomated evaluation of text generation systems\nLLMs have also been leveraged to automatically\nevaluate other aspects of generated text fragments,\nbeyond factuality. For instance, GPTScore (Fu\net al., 2023) uses a prompt that specifies the consid-\nered aspect (e.g. fluency) and then scores passages']","One strategy to assess confidence levels in tokens for factuality and truthfulness in language generation models is to inspect the probabilities assigned to individual tokens. The model is expected to be less confident in hallucinated answers compared to factual ones. BARTScore estimates factuality by looking at the conditional probability of the generated text given the input, while Kadavath et al. use a variation of this idea. Azaria and Mitchell propose training a supervised classifier on the weights from one of the hidden layers of the LLM to predict whether a given statement is true or not.",multi_context,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 1}, {'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 1}]",True
Why is it important for RAG systems to maintain context in answers and how does it impact output quality?,"['we usually do not have access to human-annotated\ndatasets or reference answers. We therefore fo-\ncus on metrics that are fully self-contained and\nreference-free. We focus in particular three quality\naspects, which we argue are of central importance.\nFirst, Faithfulness refers to the idea that the an-\nswer should be grounded in the given context. This\nis important to avoid hallucinations, and to ensure\nthat the retrieved context can act as a justification\nfor the generated answer. Indeed, RAG systems are\noften used in applications where the factual con-\nsistency of the generated text w.r.t. the grounded\nsources is highly important, e.g. in domains such as\nlaw, where information is constantly evolving. Sec-\nond,Answer Relevance refers to the idea that the\ngenerated answer should address the actual ques-\ntion that was provided. Finally, Context Relevance\nrefers to the idea that the retrieved context should\nbe focused, containing as little irrelevant informa-']","Maintaining context in answers is important for RAG systems because it ensures faithfulness, which means that the answer is grounded in the given context. This helps avoid hallucinations and ensures that the retrieved context can act as a justification for the generated answer. Additionally, maintaining context helps in ensuring factual consistency of the generated text with the grounded sources, which is crucial in domains like law where information is constantly evolving. It also impacts output quality by ensuring that the generated answer is relevant to the actual question provided and that the retrieved context is focused and contains minimal irrelevant information.",multi_context,"[{'source': 'RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf', 'page': 2}]",True
